# RQVAE config for Amazon dataset

import genrec.data.amazon
import genrec.models.rqvae

# Training settings (epoch-based with warmup)
train.epochs=5000
train.warmup_epochs=50
train.learning_rate=0.001
train.weight_decay=0.0001
train.batch_size=1024

# Model architecture
train.vae_input_dim=768
train.vae_n_cat_feats=0
train.vae_hidden_dims=[512, 256, 128, 64]
train.vae_embed_dim=32
train.vae_codebook_size=256
train.vae_codebook_normalize=False
train.vae_sim_vq=False
train.vae_n_layers=3

# Quantization mode (STE + Sinkhorn on last layer)
train.vae_codebook_mode=%genrec.models.rqvae.QuantizeForwardMode.STE
train.vae_codebook_last_layer_mode=%genrec.models.rqvae.QuantizeForwardMode.SINKHORN
train.commitment_weight=0.25

# Dataset - auto downloads if not present
train.dataset=@AmazonItemDataset
train.dataset_folder="dataset/amazon"
train.encoder_model_name="/root/workspace/models_hub/sentence-t5-xl"
AmazonItemDataset.split="beauty"

# Checkpointing (every 50 epochs, eval every 50 epochs)
train.save_model_every=50
train.eval_every=50
train.save_dir_root="out/rqvae/amazon/beauty"

# Logging
train.wandb_logging=True
train.wandb_project="amazon_beauty_rqvae_training"
train.wandb_log_interval=100
train.do_eval=True
