# LCRec config for Amazon dataset
# LCRec: Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation

include "config/base.gin"

import genrec.data.amazon_lcrec
import genrec.models.lcrec

# =============================================================================
# Training Settings
# =============================================================================
train.epochs = 4
train.batch_size = 32
train.learning_rate = 2e-5
train.weight_decay = 0.01
train.warmup_ratio = 0.01
train.gradient_accumulate_every = 2
train.max_length = 512

# =============================================================================
# Model Settings
# =============================================================================
train.pretrained_path = %MODEL_HUB_QWEN3_1_7B
train.use_lora = False

# =============================================================================
# Codebook Settings
# =============================================================================
train.num_codebooks = 5
train.codebook_size = 256

# =============================================================================
# Dataset Settings
# =============================================================================
train.dataset = @AmazonLCRecDataset
train.dataset_folder = "dataset/amazon"
train.max_seq_len = 20
train.max_text_len = 128
train.pretrained_rqvae_path = "./out/lcrec/amazon/{split}/rqvae/checkpoint_epoch_1999.pt"

AmazonLCRecDataset.split = "{split}"
AmazonLCRecDataset.encoder_model_name = %MODEL_HUB_SENTENCE_T5_XL
AmazonLCRecDataset.rqvae_input_dim = 768
AmazonLCRecDataset.rqvae_embed_dim = 64
AmazonLCRecDataset.rqvae_hidden_dims = [512, 256, 128]
AmazonLCRecDataset.rqvae_codebook_size = 256
AmazonLCRecDataset.rqvae_n_layers = 5
AmazonLCRecDataset.enabled_tasks = ["seqrec", "item2index", "index2item"]

# =============================================================================
# Evaluation Settings
# =============================================================================
train.do_eval = True
train.eval_every_epoch = 1
train.eval_batch_size = 4
train.eval_beam_width = 10

# =============================================================================
# Checkpoint Settings
# =============================================================================
train.save_dir_root = "out/lcrec/amazon/{split}"
train.save_every_epoch = 1

# =============================================================================
# Logging Settings
# =============================================================================
train.wandb_logging = True
train.wandb_project = "lcrec_{split}_training"
train.wandb_log_interval = 10
train.debug_logging = False

# =============================================================================
# Accelerator Settings
# =============================================================================
train.amp = True
train.mixed_precision_type = "bf16"
