# RQVAE config for TIGER (Amazon dataset)

include "config/base.gin"

import genrec.data.amazon
import genrec.models.rqvae

# Training settings (epoch-based with warmup)
train.epochs = 5000
train.warmup_epochs = 50
train.learning_rate = 0.001
train.weight_decay = 0.0001
train.batch_size = 1024

# Model architecture
train.vae_input_dim = 768
train.vae_n_cat_feats = 0
train.vae_hidden_dims = [512, 256, 128, 64]
train.vae_embed_dim = 32
train.vae_codebook_size = 256
train.vae_codebook_normalize = False
train.vae_sim_vq = False
train.vae_n_layers = 3

# Quantization mode (STE + Sinkhorn on last layer)
train.vae_codebook_mode = %genrec.models.rqvae.QuantizeForwardMode.STE
train.vae_codebook_last_layer_mode = %genrec.models.rqvae.QuantizeForwardMode.SINKHORN
train.commitment_weight = 0.25

# Dataset - auto downloads if not present
train.dataset = @AmazonItemDataset
train.dataset_folder = "dataset/amazon"
train.encoder_model_name = %MODEL_HUB_SENTENCE_T5_XL
AmazonItemDataset.split = "beauty"

# Checkpointing
train.save_model_every = 50
train.eval_every = 50
train.save_dir_root = "out/tiger/amazon/beauty/rqvae"

# Logging
train.wandb_logging = True
train.wandb_project = "tiger_rqvae_training"
train.wandb_log_interval = 100
train.do_eval = True
